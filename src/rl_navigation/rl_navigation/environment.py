#!/usr/bin/env python3
import rclpy
from rclpy.node import Node
from rclpy.parameter import Parameter
from geometry_msgs.msg import Twist
from nav_msgs.msg import Odometry
from sensor_msgs.msg import LaserScan
from std_srvs.srv import Empty
from gazebo_msgs.srv import SetEntityState
from gazebo_msgs.msg import EntityState
import numpy as np
import math
import time
from collections import deque 

class GazeboEnvironment(Node):
    def __init__(self, node_name='rl_env'):
        super().__init__(node_name)
        self.vel_pub = self.create_publisher(Twist, '/cmd_vel', 10)
        self.create_subscription(Odometry, '/odom', self.odom_callback, 10)
        self.create_subscription(LaserScan, '/scan', self.scan_callback, 10)

        self.position = None
        self.orientation = None
        
        self.progress_win = deque(maxlen=20)    # ç”¨äºæ£€æµ‹â€œå¡ä½â€ï¼ˆçº¦0.2~0.5sçª—å£ï¼Œéšstepé¢‘ç‡è€Œå®šï¼‰
        self.last_action = None   # 0=å‰è¿›,1=å·¦,2=å³
        self.min_distance = float('inf')
        self.goal = (3.0, 0.0)  # ç»ˆç‚¹åæ ‡
        self.prev_distance = None
        self.max_steps = 1000
        self.step_count = 0

        # ... GazeboEnvironment.__init__(...)
        # â­ å®‰å…¨å£°æ˜ + èµ‹å€¼ï¼ˆé¿å…é‡å¤å£°æ˜å¼‚å¸¸ï¼‰
        try:
            if not self.has_parameter("use_sim_time"):
                self.declare_parameter("use_sim_time", True)
        except Exception:
            # æŸäº›å‘è¡Œç‰ˆæ²¡æœ‰ has_parameter() æˆ–å…¶å®ƒåŸå› ï¼Œç›´æ¥å¿½ç•¥é‡å¤å£°æ˜å¼‚å¸¸
            try:
                self.declare_parameter("use_sim_time", True)
            except rclpy.exceptions.ParameterAlreadyDeclaredException:
                pass

        # ä¸ç®¡æ˜¯å¦æ˜¯æ–°å£°æ˜çš„ï¼Œéƒ½æŠŠå€¼è®¾ä¸º True
        self.set_parameters([Parameter("use_sim_time", Parameter.Type.BOOL, True)])

        # Gazebo service client
        self.set_state_client = self.create_client(SetEntityState, '/gazebo/set_entity_state')
        self.reset_client = self.create_client(Empty, '/reset_simulation')
        self.get_logger().info(f"{node_name} node activated")

        self.episode_crashes = 0
        self.episode_success = False
        self.last_reason = "continue"  # reach / crush / overtime / continue


        


    def odom_callback(self, msg):
        self.position = msg.pose.pose.position
        orientation_q = msg.pose.pose.orientation
        self.orientation = self.quaternion_to_yaw(orientation_q)

    def scan_callback(self, msg):
        scan = np.array(msg.ranges)
        scan = np.clip(scan, 0.0, 3.5)  # Clip scan range
        self.min_distance = np.min(scan)
        self.scan_state = scan[::len(scan)//10]  # Downsample to 10 beams

    def quaternion_to_yaw(self, q):
        siny_cosp = 2 * (q.w * q.z + q.x * q.y)
        cosy_cosp = 1 - 2 * (q.y * q.y + q.z * q.z)
        return math.atan2(siny_cosp, cosy_cosp)

    def get_observation(self):
        if self.position is None or self.orientation is None:
            return None
        return np.concatenate([
            np.array([self.position.x, self.position.y, self.orientation]),
            self.scan_state
        ])

    def compute_distance_to_goal(self):
        dx = self.goal[0] - self.position.x
        dy = self.goal[1] - self.position.y
        return math.hypot(dx, dy)

    # def reset(self):
    #     # Reset Gazebo world
    #     self.get_logger().info('Resetting the Gazebo world...')
    #     while not self.reset_client.wait_for_service(timeout_sec=1.0):
    #         self.get_logger().info('Waiting for /reset_world service...')
    #     req = Empty.Request()
    #     future = self.reset_client.call_async(req)
    #     rclpy.spin_until_future_complete(self, future)
    #     self.get_logger().info('World reset completed.')

    #     # Wait for odom and scan to update
    #     self.get_logger().info('Waiting for initial sensor data...')
    #     while rclpy.ok() and (self.position is None or self.orientation is None or self.min_distance == float('inf')):
    #         rclpy.spin_once(self, timeout_sec=0.1)
    #     self.get_logger().info('Initial sensor data received.')


    #     self.prev_distance = None
    #     self.step_count = 0
    #     return self.get_observation()

    def reset(self):
        # Reset Gazebo world
        self.get_logger().info('Resetting the Gazebo world...')
        while not self.reset_client.wait_for_service(timeout_sec=1.0):
            self.get_logger().info('Waiting for /reset_world service...')
        req = Empty.Request()
        future = self.reset_client.call_async(req)
        rclpy.spin_until_future_complete(self, future)
        self.get_logger().info('World reset completed.')

        # ğŸŸ¡ å¼ºåˆ¶æ¸…ç©ºæ‰€æœ‰çŠ¶æ€ï¼ˆéå¸¸å…³é”®ï¼‰
        self.position = None
        self.orientation = None
        self.min_distance = float('inf')   # âœ… æ²¡æœ‰è¿™è¡Œå¯èƒ½å¯¼è‡´åˆšå¼€å§‹å°±è§¦å‘ç¢°æ’done
        self.prev_distance = None
        self.step_count = 0

        # ğŸŸ¢ ç­‰å¾…æ–°çš„ä¼ æ„Ÿå™¨æ•°æ®
        self.get_logger().info('Waiting for initial sensor data...')
        while rclpy.ok() and (self.position is None or self.orientation is None or self.min_distance == float('inf')):
            rclpy.spin_once(self, timeout_sec=0.1)
        self.get_logger().info('Initial sensor data received.')

        # ğŸ”´ [å¯é€‰] æ£€æŸ¥æ˜¯å¦åˆå§‹å°±å·²åˆ°è¾¾ç»ˆç‚¹ï¼Œé¿å…doneç«‹å³è§¦å‘
        current_distance = self.compute_distance_to_goal()
        if current_distance < 0.3:
            self.get_logger().warn(f"Spawned too close to goal! Distance={current_distance:.2f}")


        self.episode_crashes = 0
        self.episode_success = False
        self.last_reason = "continue"  # reach / crush / overtime / continue


        return self.get_observation()



    # def base_step(self, linear, angular, duration=1.0):
    #     cmd = Twist()
    #     cmd.linear.x = linear
    #     cmd.angular.z = angular
    #     self.vel_pub.publish(cmd)
    #     rclpy.spin_once(self, timeout_sec=duration)
    #     self.vel_pub.publish(Twist())  # Stop after action


    def base_step(self, linear, angular, duration_sim=0.5, control_hz=40):
        """
        duration_sim: æœ¬æ¬¡åŠ¨ä½œæŒç»­çš„ã€ä»¿çœŸæ—¶é—´ã€‘ç§’ï¼ˆä¸æ˜¯å¢™é’Ÿï¼‰
        control_hz:   æ§åˆ¶é¢‘ç‡ï¼ˆæ¯ç§’å‘å¸ƒå¤šå°‘æ¬¡é€Ÿåº¦æŒ‡ä»¤ï¼‰
        """
        cmd = Twist()
        cmd.linear.x = linear
        cmd.angular.z = angular

        # ä»¥ä»¿çœŸæ—¶é—´ä¸ºåŸºå‡†æ¨è¿›
        start = self.get_clock().now()
        period = 1.0 / control_hz

        while (self.get_clock().now() - start).nanoseconds * 1e-9 < duration_sim:
            self.vel_pub.publish(cmd)
            # å¤„ç†æ¶ˆæ¯ï¼Œä¸ç­‰å¾…å¢™é’Ÿï¼ˆ0.0ï¼‰ï¼ŒGazebo RTF>1 æ—¶ä¼šéšä¹‹åŠ é€Ÿ
            rclpy.spin_once(self, timeout_sec=0.0)
            # ç”¨ä¸€ä¸ªå¾ˆçŸ­çš„å¢™é’Ÿå°ç¡ï¼Œé¿å…CPU 100%å¿™ç­‰ï¼›å€¼è¶Šå°è¶Šå¿«
            time.sleep(min(0.0005, period * 0.2))

        # åœè½¦ï¼Œå¹¶å¤„ç†æœ€åä¸€æ‰¹å›è°ƒ
        self.vel_pub.publish(Twist())
        rclpy.spin_once(self, timeout_sec=0.0)


    # def compute_reward(self, action, prev_action=None):
    #     """
    #     å¥–åŠ±å¡‘å½¢è¦ç‚¹ï¼š
    #     - å¼±åŒ–â€œæœå‘ç›®æ ‡â€å¥–åŠ±ï¼Œé¿å…æ­»ç›¯ç›´çº¿
    #     - åˆ©ç”¨æ¿€å…‰çš„å‰/å·¦/å³æ‰‡åŒºåšé¿éšœå¼•å¯¼
    #     * å‰æ–¹å¾ˆè¿‘ä»å‰è¿› -> å¤§æƒ©ç½š
    #     * å‘æ›´ç©ºçš„ä¸€ä¾§è½¬å¼¯ -> å°å¥–åŠ±
    #     - é™ä½åŸåœ°æ—‹è½¬æƒ©ç½šï¼Œé¼“åŠ±æ¢ç´¢
    #     """
    #     # --- è·ç¦»ä¸æœå‘ ---
    #     current_distance = self.compute_distance_to_goal()
    #     if self.prev_distance is None:
    #         self.prev_distance = current_distance
    #     distance_diff = self.prev_distance - current_distance

    #     dx = self.goal[0] - self.position.x
    #     dy = self.goal[1] - self.position.y
    #     goal_angle = math.atan2(dy, dx)
    #     heading_error = goal_angle - self.orientation
    #     heading_error = (heading_error + math.pi) % (2 * math.pi) - math.pi  # wrap [-pi, pi]

    #     # å¼±åŒ–â€œæœå‘ç›®æ ‡â€é¡¹ï¼ˆé¿å…ç›´çº¿ç¡¬é¡¶ï¼‰
    #     angle_reward = -0.3 * abs(heading_error)

    #     # åŸºç¡€å¥–åŠ±ï¼šé è¿‘ç›®æ ‡ + æœå‘ç›®æ ‡
    #     reward = distance_diff * 10.0 + angle_reward

    #     # --- åˆ©ç”¨æ¿€å…‰æ‰‡åŒºåšå±€éƒ¨é¿éšœå¼•å¯¼ ---
    #     scan = np.asarray(self.scan_state, dtype=float) if hasattr(self, "scan_state") else np.array([self.min_distance])
    #     n = len(scan)
    #     if n >= 3:
    #         mid = n // 2
    #         # å‰æ–¹å–ä¸­é—´2ä¸ªbeamçš„æœ€å°å€¼ï¼Œæ›´ç¨³å¥
    #         front = np.min(scan[max(0, mid-1):min(n, mid+1)])
    #         left  = np.min(scan[mid:]) if mid < n else float('inf')
    #         right = np.min(scan[:mid]) if mid > 0 else float('inf')
    #     else:
    #         front = self.min_distance
    #         left, right = self.min_distance, self.min_distance

    #     # 1) å‰æ–¹è¿‘éšœä»å°è¯•å‰è¿› => å¤§æƒ©ç½šï¼ˆè·ç¦»è¶Šè¿‘æƒ©ç½šè¶Šå¤§ï¼‰
    #     if action == 0 and front < 0.6:
    #         reward -= (0.6 - front) * 3.0   # ç³»æ•°å¯åœ¨ 2.0~5.0 é—´è°ƒ

    #     # 2) æœæ›´ç©ºçš„ä¸€ä¾§è½¬å¼¯ => å°å¥–åŠ±ï¼ˆåªç»™æ­£å‘å·®å€¼ï¼‰
    #     gap_bias = 0.0
    #     if action == 1:           # å·¦è½¬
    #         gap_bias = (left - right)
    #     elif action == 2:         # å³è½¬
    #         gap_bias = (right - left)
    #     reward += 0.2 * max(0.0, gap_bias)  # 0.1~0.3 å¯è°ƒ

    #     # 3) é è¿‘éšœç¢ç‰©çš„å…¨å±€æƒ©ç½šï¼ˆä¿ç•™ï¼‰
    #     if self.min_distance < 0.5:
    #         reward -= (0.5 - self.min_distance) * 2.0

    #     # 4) é™ä½æ—‹è½¬æƒ©ç½šï¼Œä¿ç•™ä¸€ç‚¹ç‚¹æƒ¯æ€§çº¦æŸ
    #     if action in [1, 2]:
    #         reward -= 0.005   # åŸæ¥æ˜¯ 0.02

    #     # --- æŠ—æŠ–åŠ¨é¡¹ï¼šå·¦å³æ¥å›åˆ‡æ¢ç»™å°æƒ©ç½š ---
    #     if prev_action in (1, 2) and action in (1, 2) and prev_action != action:
    #         reward -= 0.08   # 0.05~0.12 ä¹‹é—´å¯è°ƒï¼›å…ˆç”¨ 0.08


    #     # 5) è½»æ—¶é—´æƒ©ç½šï¼Œé¼“åŠ±é«˜æ•ˆ
    #     reward -= 0.05

    #     # --- ç»ˆæ­¢æ¡ä»¶ ---
    #     if current_distance < 0.3:
    #         reward += 30.0
    #         done = True
    #         self.last_reason = "reach"
    #         self.episode_success = True
    #         self.get_logger().info("reach the end")
    #     elif self.min_distance < 0.2:
    #         reward -= 30.0
    #         done = True
    #         self.last_reason = "crush"
    #         self.episode_crashes += 1
    #         self.get_logger().info("crush")
    #     elif self.step_count >= self.max_steps:
    #         reward -= 10.0
    #         done = True
    #         self.last_reason = "overtime"
    #         self.get_logger().info("overtime")
    #     else:
    #         done = False
    #         self.last_reason = "continue"

    #     self.prev_distance = current_distance

    #     return reward, done


    def compute_reward(self, action, prev_action=None):
        """
        å¼ºåŒ–é¿éšœ & é˜²å¡æ­» çš„å¥–åŠ±å¡‘å½¢ï¼š
        - è¿›åº¦ä¸ºä¸»ï¼Œæœå‘ä¸ºè¾…ï¼ˆå¼±åŒ–ï¼‰
        - ä¸‰æ‰‡åŒºå¼•å¯¼ï¼šå‰æ–¹è¿‘éšœ+å‰è¿›é‡ç½šï¼›è½¬å‘æ›´ç©ºä¸€ä¾§å°å¥–ï¼›å‰è¿›åè¾¹æ‰£åˆ†
        - é˜²æŠ–ï¼šå·¦å³åˆ‡æ¢å°ç½š
        - é˜²å¡æ­»ï¼šçŸ­çª—å£å†…è¿›åº¦å‡ ä¹ä¸ºé›¶ä¸”å‰æ–¹ä¸ç©º => å°ç½š
        """
        # ---------- ç›®æ ‡è¿›åº¦ & æœå‘ ----------
        cur_dist = self.compute_distance_to_goal()
        if self.prev_distance is None:
            self.prev_distance = cur_dist
        progress = self.prev_distance - cur_dist      # >0 è¡¨ç¤ºæœç›®æ ‡å‰è¿›
        self.prev_distance = cur_dist

        dx = self.goal[0] - self.position.x
        dy = self.goal[1] - self.position.y
        goal_angle = math.atan2(dy, dx)
        heading_err = (goal_angle - self.orientation + math.pi) % (2*math.pi) - math.pi

        # è¿›åº¦å¥–åŠ±ï¼ˆä¸»ï¼‰+ å¼±åŒ–æœå‘é¡¹ï¼ˆè¾…ï¼‰
        reward = 8.0 * progress + 0.25 * math.cos(heading_err)   # cos æ›´å¹³æ»‘ï¼Œæƒé‡å¤§å¹…ä½äºè¿›åº¦

        # ---------- æ¿€å…‰ä¸‰æ‰‡åŒº ----------
        scan = np.asarray(self.scan_state, dtype=float) if hasattr(self, "scan_state") else np.array([self.min_distance])
        n = len(scan)
        if n >= 5:
            mid = n // 2
            front = np.min(scan[max(0, mid-1):min(n, mid+2)])  # å–ä¸­é—´3æŸæ›´ç¨³
            left  = np.min(scan[mid:]) if mid < n else float('inf')
            right = np.min(scan[:mid]) if mid > 0 else float('inf')
        else:
            front = self.min_distance
            left = right = self.min_distance

        # é˜ˆå€¼å¯æŒ‰åœ°å›¾å†è°ƒ
        FRONT_SOFT = 0.75      # å‰æ–¹â€œæ‹¥æŒ¤â€é˜ˆå€¼ï¼ˆè½¯æƒ©ç½šï¼‰
        FRONT_HARD = 0.55      # å‰è¿›ç¦åŒºé˜ˆå€¼ï¼ˆç¡¬æƒ©ç½šæ›´å¼ºï¼‰
        SIDE_SAFE  = 0.35      # é è¾¹é˜ˆå€¼ï¼ˆå‰è¿›æ—¶ç¦»ä¸€ä¾§å¤ªè¿‘å°±æ‰£åˆ†ï¼‰

        # 1) å‰æ–¹è¿‘éšœä»å‰è¿› => é‡ç½šï¼ˆç¡¬/è½¯ä¸¤æ¡£ï¼‰
        if action == 0 and front < FRONT_SOFT:
            k = 5.0 if front < FRONT_HARD else 3.0
            reward -= k * (FRONT_SOFT - front)  # ç¦»å¾—è¶Šè¿‘ï¼Œæ‰£å¾—è¶Šå¤š

        # 2) å‘æ›´ç©ºçš„ä¸€ä¾§è½¬å¼¯ => å°å¥–åŠ±ï¼ˆåªç»™æ­£å‘å·®å€¼ï¼‰
        if action in (1, 2):
            gap_bias = (left - right) if action == 1 else (right - left)
            reward += 0.25 * max(0.0, gap_bias)   # 0.15~0.35 å¯è°ƒ

        # 3) å‰è¿›æ—¶â€œåè¾¹â€æ‰£åˆ†ï¼ˆé¼“åŠ±å±…ä¸­é€šè¿‡ï¼‰
        if action == 0:
            side_min = min(left, right)
            # ç¦»å¢™è¿‡è¿‘æ—¶çº¿æ€§æ‰£åˆ†ï¼›ä¹Ÿå¯ç”¨ |left-right| æƒ©ç½šåä½ï¼Œè¿™é‡Œé€‰â€œå®‰å…¨è·ç¦»â€ä¼˜å…ˆ
            if side_min < SIDE_SAFE:
                reward -= 1.5 * (SIDE_SAFE - side_min)

        # 4) å…¨å±€å®‰å…¨æ„Ÿï¼ˆä¸æœ€è¿‘éšœç¢çš„è·ç¦»ï¼‰â€”â€”æ¸©å’Œæƒ©ç½š
        if self.min_distance < 0.50:
            reward -= 2.0 * (0.50 - self.min_distance)

        # ---------- é˜²æŠ–ï¼ˆå·¦å³æ¥å›åˆ‡æ¢ç½šï¼‰ ----------
        if prev_action in (1, 2) and action in (1, 2) and prev_action != action:
            reward -= 0.10   # 0.08~0.12ï¼›æŠ–å¾—ç‹ å°±åŠ å¤§

        # ---------- é˜²å¡æ­»ï¼ˆçŸ­çª—å£å†…è¿›åº¦â‰ˆ0 ä¸”å‰æ–¹ä¸ç©ºï¼‰ ----------
        self.progress_win.append(progress)
        if len(self.progress_win) == self.progress_win.maxlen:
            win_prog = sum(self.progress_win)
            # å¦‚æœçª—å£å†…å‡ ä¹æ²¡è¿›æ­¥ + å‰é¢ä¸æ˜¯â€œå®Œå…¨é€šç•…â€ï¼Œç»™å°ç½šæ¨åŠ¨ç­–ç•¥åˆ‡æ¢
            if win_prog < 0.02 and front < 1.0:
                reward -= 0.20   # å¯é€æ­¥é€’å¢ï¼ˆæ¯”å¦‚å†å¤šæ¬¡è§¦å‘åˆ™å åŠ ï¼‰ï¼Œæ­¤å¤„å…ˆå›ºå®š

        # ---------- è½»æ—¶é—´æƒ©ç½š ----------
        reward -= 0.03

        # ---------- ç»ˆæ­¢åˆ¤å®š ----------
        if cur_dist < 0.30:
            reward += 30.0
            done = True
            self.last_reason = "reach"
            self.episode_success = True
            self.get_logger().info("reach the end")
        elif self.min_distance < 0.20:
            reward -= 30.0
            done = True
            self.last_reason = "crush"
            self.episode_crashes += 1
            self.get_logger().info("crush")
        elif self.step_count >= self.max_steps:
            reward -= 10.0
            done = True
            self.last_reason = "overtime"
            self.get_logger().info("overtime")
        else:
            done = False
            self.last_reason = "continue"

        return reward, done


    def is_violation(self):
        return self.min_distance < 0.2


# class NoMonitoringEnv(GazeboEnvironment):
#     def step(self, action):
#         """Discrete action: 0=forward, 1=left, 2=right"""
#         if action == 0:
#             self.base_step(0.4, 0.0)
#         elif action == 1:
#             self.base_step(0.0, 0.5)
#         elif action == 2:
#             self.base_step(0.0, -0.5)

#         self.step_count += 1
#         reward, done = self.compute_reward(action)
#         obs = self.get_observation()
#         return obs, reward, done, {}

class NoMonitoringEnv(GazeboEnvironment):
    def step(self, action):
        if action == 0:  # å‰è¿›
            self.base_step(0.4, 0.0,  duration_sim=0.25, control_hz=40)
        elif action == 1:  # å·¦è½¬
            self.base_step(0.0, 0.6,  duration_sim=0.20, control_hz=40)
        elif action == 2:  # å³è½¬
            self.base_step(0.0, -0.6, duration_sim=0.20, control_hz=40)

        self.step_count += 1

        # ...
        prev_action = self.last_action          # è®°ä¸€ä¸‹ä¹‹å‰çš„åŠ¨ä½œ
        reward, done = self.compute_reward(action, prev_action)  # ä¼ å…¥ä¸Šä¸€æ­¥åŠ¨ä½œ
        self.last_action = action               # æ›´æ–°ä¸ºå½“å‰åŠ¨ä½œ
        # ...
        obs = self.get_observation()

        info = {
            "reason": self.last_reason,                           # reach/crush/overtime/continue
            "distance_to_goal": float(self.compute_distance_to_goal()),
            "min_laser": float(self.min_distance),
            "step_count": int(self.step_count),
            "violation": bool(self.is_violation()),
            "success": bool(self.episode_success),
            "crashes_in_episode": int(self.episode_crashes),
        }


        return obs, reward, done, info



class PassiveMonitoringEnv(GazeboEnvironment):
    def step(self, action):
        if self.is_violation():
            self.get_logger().warn("Passive Monitor: Violation detected!")
        return super().step(action)


class ActiveMonitoringEnv(GazeboEnvironment):
    def step(self, action):
        if self.is_violation() and action == 0:  # Trying to move forward into obstacle
            self.get_logger().warn("Active Monitor: Stopping action due to violation!")
            action = 1  # Replace with turn left
        return super().step(action)
